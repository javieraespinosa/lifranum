{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawl and WARC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1gPNBrF8m4QuLSuEzsputGtZa9Gyq6ids",
      "authorship_tag": "ABX9TyOIBHP0sVdz77xCSTkuhIsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javieraespinosa/lifranum/blob/main/Crawl_and_WARC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGowCMee1qvr"
      },
      "source": [
        "# Crawling and Archiving Websites\n",
        "\n",
        "This notebook illustrates the use of [Wget](https://www.gnu.org/software/wget/manual/wget.html) for crawling and archiving websites using the [WARC file format](https://en.wikipedia.org/wiki/Web_ARChive).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRyXDQid0Aoa"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-Z2Qh-qzzt"
      },
      "source": [
        "## Updating Wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRk1VhJ4IMbu"
      },
      "source": [
        "Wget supports the production of WARC files since v1.14. Colab includes Wget v1.19.4 by default, but this version does not support WARC compression. The following code upgrades Wget to a version supporting WARC per-record and file-level compression, which considerably reduces storage space usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Domp9Of1FaZ"
      },
      "source": [
        "TMP_DIR = \"tmp\"\n",
        "\n",
        "!mkdir {TMP_DIR}\n",
        "%cd {TMP_DIR}\n",
        "\n",
        "!wget -nv  http://ftp.gnu.org/gnu/wget/wget-1.21.tar.gz\n",
        "!tar -xzf wget-1.21.tar.gz\n",
        "\n",
        "!./wget-1.21/configure --quiet --with-ssl=openssl > /dev/null 2>&1 \n",
        "\n",
        "!make > log.txt 2>&1   \n",
        "!make install > log.txt 2>&1 \n",
        "\n",
        "%cd ..\n",
        "!rm -r {TMP_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV1snH0BnMhJ"
      },
      "source": [
        "## Wget General Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsGISmCQqZLx"
      },
      "source": [
        "The crawling examples in this notebook assume the existence of an `input.txt` file containing the list of URLs to crawl. You can modify the content of this file according to your needs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJgVrz6avOE-"
      },
      "source": [
        "!echo \"http://example.com/\" > input.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XibaHpnng9S"
      },
      "source": [
        "The following variables define Wget' **default behaviour** when crawling. You can modify them at will, either here or directly in the cell executing `!wget`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQN0m74YFOK5"
      },
      "source": [
        "LEVEL=1       # maximum number of links to follow (i.e, crawl depth)\n",
        "WAIT=0.1      # num. seconds to wait between consecutive calls \n",
        "\n",
        "INPUT_FILE = \"input.txt\"     # list of URLs to crawl\n",
        "\n",
        "OUT_DIR       = \"WARC\"       # folder where crawl results will be stored\n",
        "OUT_WARC_FILE = \"out\"        # prefix for WARC files\n",
        "OUT_LOG_FILE  = \"log.txt\"    # file containing Wget's log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-fV6OmWz5xh"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfFsRIQ0Rz9"
      },
      "source": [
        "## Ex1. Crawling a specific domain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKB_wUIWR8L8"
      },
      "source": [
        "The following example crawls the URLs in `input.txt` and produces a WARC file containing only HTML files (i.e., wget ignores images, css, js and all other files). By default, wget will only follow the URLs within the same domain. This behaviour is useful for crawling the entirety of a specific domain.\n",
        "\n",
        "> Recall that wget will stop after following links 1 level deep. To crawl the entirety of a domain set `LEVEL=0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ZZ3WQyrcyk"
      },
      "source": [
        "LEVEL=1\n",
        "!wget \\\n",
        "  --delete-after -nd \\\n",
        "  --input-file={INPUT_FILE}  \\\n",
        "  --recursive   \\\n",
        "  --level={LEVEL}     \\\n",
        "  --no-parent   \\\n",
        "  --wait={WAIT}    \\\n",
        "  --random-wait   \\\n",
        "  --follow-tags=a \\\n",
        "  --accept html  \\\n",
        "  --adjust-extension \\\n",
        "  --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\" \\\n",
        "  --warc-file={OUT_WARC_FILE}  \\\n",
        "  --warc-cdx=on \\\n",
        "  --warc-max-size=1G  \\\n",
        "  --no-warc-keep-log  \\\n",
        "  --output-file={OUT_LOG_FILE} \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsOnwAxGfBWu"
      },
      "source": [
        "# Move resulting files to the OUT_DIR folder\n",
        "!mkdir -p {OUT_DIR} \n",
        "!mv *.warc.gz *.cdx {OUT_LOG_FILE} {OUT_DIR} \n",
        "!cp {INPUT_FILE} {OUT_DIR}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQwVR7gRQ33F"
      },
      "source": [
        "## Ex2. Multi-host crawling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0k4u2JmRqLS"
      },
      "source": [
        "This example mimics the previous crawling but follows links pointing to other hosts.\n",
        "\n",
        "> **Attention!** If `LEVEL=0` wget will crawl the entire web! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EUjXU-xQ_mK"
      },
      "source": [
        "LEVEL=1\n",
        "!wget \\\n",
        "  -H  \\    # Span to any hos\n",
        "  --delete-after -nd \\\n",
        "  --input-file={INPUT_FILE}  \\\n",
        "  --recursive   \\\n",
        "  --level={LEVEL}     \\\n",
        "  --no-parent   \\\n",
        "  --wait={WAIT}    \\\n",
        "  --random-wait   \\\n",
        "  --follow-tags=a \\\n",
        "  --accept html  \\\n",
        "  --adjust-extension \\\n",
        "  --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\" \\\n",
        "  --warc-file={OUT_WARC_FILE}  \\\n",
        "  --warc-cdx=on \\\n",
        "  --warc-max-size=1G  \\\n",
        "  --no-warc-keep-log  \\\n",
        "  --output-file={OUT_LOG_FILE} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq8lAkGMSi_O"
      },
      "source": [
        "# Move resulting files to the OUT_DIR folder\n",
        "!mkdir -p {OUT_DIR} \n",
        "!mv *.warc.gz *.cdx {OUT_LOG_FILE} {OUT_DIR} \n",
        "!cp {INPUT_FILE} {OUT_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}